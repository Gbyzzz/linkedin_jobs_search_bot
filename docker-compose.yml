version: "3.9"
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "22181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  bot_postgres:
    container_name: bot_postgres
    image: postgres:16.3
    ports:
      - "5433:5432"
    restart: always
    env_file:
      - .env
    depends_on:
      - "zookeeper"
      - "kafka"
    environment:
      POSTGRES_PASSWORD: ${PASSWORD}
      POSTGRES_DB: linkedin_jobs_bot
  #    volumes:
  #      - postgres-data:/var/lib/postgresql/data

  bot_redis:
    container_name: bot_redis
    image: redis:latest
    env_file:
      - .env
    ports:
      - "7917:6379" # Port 7917 on host mapped to 6379 inside container (standard Redis port)
    environment:
      REDIS_PASSWORD: ${PASSWORD}
    restart: always

  bot_mongodb:
    container_name: bot_mongodb
    image: mongodb/mongodb-community-server:latest
    command: mongod --port 27018
    env_file:
      - .env
    ports:
      - "27018:27018"
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${PASSWORD}
    restart: always

  bot_main:
    build: ./linkedin-jobs-bot-main
    restart: always
    env_file:
      - .env
    container_name: bot_main
    depends_on:
      - "bot_postgres"
      - "bot_redis"
      - "bot_mongodb"
      - "zookeeper"
      - "kafka"
    ports:
      - "8888:8080"
    working_dir: /app
    environment:
      - SPRING_DATASOURCE_URL=jdbc:postgresql://bot_postgres:5432/linkedin_jobs_bot
      - SPRING_DATASOURCE_PASSWORD=${PASSWORD}
      - SPRING_DATA_MONGODB_HOST=bot_mongodb
      - SPRING_DATA_MONGODB_PORT=27018
      - SPRING_DATA_MONGODB_DATABASE=jobs_db
      - SPRING_DATA_MONGODB_PASSWORD=${PASSWORD}
      - SPRING_DATA_MONGODB_USERNAME=${USERNAME}
      - SPRING_DATA_REDIS_HOST=bot_redis
      - SPRING_DATA_REDIS_PASSWORD=${PASSWORD}
      - BOT_NAME=${BOT_NAME}
      - BOT_TOKEN=${BOT_TOKEN}
      - SPRING_KAFKA_BOOTSTRAP-SERVERS=kafka:9092

  bot_scanner_one:
    build: ./job-scanner
    restart: always
    env_file:
      - .env
    container_name: bot_scanner_one
    depends_on:
      - "bot_postgres"
      - "bot_redis"
      - "bot_mongodb"
      - "zookeeper"
      - "kafka"
      - "bot_main"
    ports:
      - "8881:8080"
    working_dir: /app
    environment:
      - SPRING_DATA_MONGODB_HOST=bot_mongodb
      - SPRING_DATA_MONGODB_PORT=27018
      - SPRING_DATA_MONGODB_DATABASE=jobs_db
      - SPRING_DATA_MONGODB_PASSWORD=${PASSWORD}
      - SPRING_DATA_MONGODB_USERNAME=${USERNAME}
      - BOT_USERNAME=${BOT_USERNAME_ONE}
      - BOT_PASSWORD=${BOT_PASSWORD_ONE}
      - BOT_COOKIE=${BOT_COOKIE_ONE}
      - BOT_CSRF_TOKEN=${BOT_CSRF_TOKEN_ONE}
      - SPRING_KAFKA_BOOTSTRAP-SERVERS=kafka:9092

  bot_scanner_two:
    build: ./job-scanner
    restart: always
    env_file:
      - .env
    container_name: bot_scanner_two
    depends_on:
      - "bot_postgres"
      - "bot_redis"
      - "bot_mongodb"
      - "zookeeper"
      - "kafka"
      - "bot_main"
    ports:
      - "8882:8080"
    working_dir: /app
    environment:
      - SPRING_DATA_MONGODB_HOST=bot_mongodb
      - SPRING_DATA_MONGODB_PORT=27018
      - SPRING_DATA_MONGODB_DATABASE=jobs_db
      - SPRING_DATA_MONGODB_PASSWORD=${PASSWORD}
      - SPRING_DATA_MONGODB_USERNAME=${USERNAME}
      - BOT_USERNAME=${BOT_USERNAME_TWO}
      - BOT_PASSWORD=${BOT_PASSWORD_TWO}
      - BOT_COOKIE=${BOT_COOKIE_TWO}
      - BOT_CSRF_TOKEN=${BOT_CSRF_TOKEN_TWO}
      - SPRING_KAFKA_BOOTSTRAP-SERVERS=kafka:9092

#volumes:
#  postgres-data:
#    volumes:
#        - ./create-topics.sh:/create-topics.sh
#    command: [ "/bin/bash", "/create-topics.sh" ]
#    command: "bash -c 'echo Waiting for Kafka to be ready... && \
#                           cub kafka-ready -b kafka:9092 1 20 && \
#                           kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 2 --replication-factor 1 --topic to_search && \
#                           echo Waiting 60 seconds for Connect to be ready... && \
#                           sleep 60 && \
#                           curl -i -X POST -H Accept:application/json -H Content-Type:application/json http://connect:8083/connectors/ -d @/tmp/connectors/connector_elasticsearch_docker.config && \
#                           curl -i -X POST -H Accept:application/json -H Content-Type:application/json http://connect:8083/connectors/ -d @/tmp/connectors/connector_jdbc_customers_docker.config && \
#                           echo Waiting 90 seconds for Elasticsearch and Kibana to be ready... && \
#                           sleep 90 && \
#                           /tmp/dashboard/docker-combined.sh'"
#    command: "
#     # blocks until kafka is reachable
#      kafka-topics --bootstrap-server kafka:9092 --list
#
#      echo -e 'Creating kafka topics'
#      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic to_search --replication-factor 1 --partitions 2
#
#      echo -e 'Successfully created the following topics:'
#      kafka-topics --bootstrap-server kafka:9092 --list
#    "